\section{Optimal Control Genome Method}
\begin{figure*}
    \centering
    \includegraphics[width=0.75\linewidth]{img/overview}
    \caption{OMGkj}
    \label{fig:movement:overview}
\end{figure*}
Human movement is a complex interplay between intentions within the brain and the physical environment. The thought to 'walk-forward' triggers and array of intractable optimizations. 

In the following we will introduce a method for synchronization between a Movement Model and a set of animations given Control Genomes. There are 4 main parts to the method. 
\begin{itemize}
    \item \textbf{Control Genomes} extracted from animations.
    \item \textbf{Movement Model} that generates a movement given Control Genomes.
    \item \textbf{Animation Alignment} and filtering. 
    \item \textbf{Optimization procedure} that fits exposed parameters of all the above systems to minimize the difference between the Movement Model- and animation trajectories.
\end{itemize}
Figure \ref{fig:movement:overview} illustrates the definitions and concepts of a movement model.

\subsection{Control Genomes}
We define Control Genomes as signals that can generate movement. They are a combination of intentions and enough contextual state to follow the Markov principle. As an example a position, direction and time is a control genome for a straight walk. Conversely a straight walk contains a latent control genome (position, direction, time). Under this naive framework we quickly realize that multiple straight walks could be associated with a single control genome. We further impose the constraint that control genomes must disambiguate movement. That means a unique control genome maps to a unique movement. In our simple example we might resolve conflicts by adding a style parameter and assign values such as 'brisk walk' or 'dragging feet' to our control genomes. 

Control genomes can have a direct counter part in the host application such as user input through a game controller or navigational path from an AI system, and therefore can also be viewed as tasks that are carried out by the animations, for instance turn to right. In general we want the control genomes to be of minimal size, since in the limit we could have the animation itself as the control genome. We say that a genome is in \textit{reduced} and \textit{segmented} form if it contains no repetitions and removal of information would break either the Markov property of the disambiguation constraint. 

\begin{figure}
    \centering
    \includegraphics[width=1\columnwidth]{img/controlgenome.png}
    \caption{Control Genome.}
    \label{fig:control:genome}
\end{figure}

Figure \ref{fig:control:genome} shows an example where an animation has been assigned a control genome. Each frame is annotated with a 2D-control direction corresponding to stick input from a game controller. A dense and redundant control genome contains the entire skeletal animation as well as the annotated directions. The animation has two similar right turns so we get a single segmented control genome by splitting the redundant genome in two identical parts. We then trim all unnecessary information from the genome to get the reduced form which only contain a starting position and two direction offset in time. This last step assumes that we are capable of regenerating the animation from the reduced genome using a model we will refer to as a \textit{Movement Model}. A Control Genome paired with a Movement Model describes an animation exactly when the Movement Model can regenerate the Animation from the Control Genome.  In practice it is not possible to develop accurate generative or predictive models for human movement. So we will allow our animations to undergo non destructive transformation such as smoothing and path adjustment, and only expect our model to regenerate a fraction of the complete signal, such as the trajectory. 

We will now proceed to a formalized description of control genomes. The input to our method is an annotated animation. Let $\anim^{\dimas,\dimaa}_\dimat$ denote $\dimat$ frames of animation of a skeleton with $\dimas$ degrees of freedom, where each frame is given an $\dimaa$-dimensional annotation. 

We assume there exist an automated or manual Retrieve-and-Collapse function, $\reco$,  that extracts all reduced and segmented control genomes present in the animation and for each maintains a pairing to all $k$ corresponding un-annotated segments of the source animation. That is $\reco$ converts a source animation into an associate map, $\lut$, that uses genomes as keys and un-annotated segments of source animation as values. We may write $\reco(\anim^{\dimas,\dimaa}_\dimat) \rightarrow \lut$. Now given the $i\th$ control genome of dimension $\dimg$ we have
\begin{equation}
 \lut(\genome^g_i) \rightarrow \left\{\anim^{\dimas,\dimaa}_{\dimae_0},\ldots,\anim^{\dimas,\dimaa}_{\dimae_k}\right\}   \,,
\end{equation}
where $\dimae\ll\dimat$ refers to a segment of the full animation and $k$ is the number of segments matched to the $i\th$ control genome.

We define two parametric projections, $\model$ and $\edit$, to a shared $\dimes$-dimensional space, $\mathcal{R}^{\dimes}$, as follows
\begin{align}
\model(\paramm,\genome_i^{\dimg}) 
&\rightarrow 
\mathcal{R}^{\dimes}    \,,
\\
\edit(\parame,\anim_{\dimae}^{\dimas,\dimaa})
&\rightarrow
\mathcal{R}^{\dimes} \,,
\end{align}
where $\paramm$ and $\parame$ are free parameters to be optimized for later. We call $\mathcal{R}^{\dimes}$ an evaluation space and it will usually have a natural counterpart in the application such as the trajectory (position and orientation of character over time).
u
Intuitively $\model$ is a Movement Model capable of generating movement or more accurately an evaluation space representation from the control genomes. $\edit$ corresponds to the adjustment we allow our animations to undergo, such a smoothing or more advanced manipulation. The optimal parameters for a single control genome are found as the minimization of the L2-norm.
\begin{subequations}
\begin{align}
    \gnorm(\paramm,\parame,\genome_i)
    &\equiv
    \sum_{\anim_k\in\lut(\genome_i)}
    {
        \frac{1}{2}
        |\model(\paramm,\genome_i)
        -
        \edit(\parame\anim_k)|^2
    }
    \label{eq:optim:single}
    \\
    \paramm^*,\parame^*
    &\equiv \arg\min_{\paramm,\parame}
    {
        \gnorm(\paramm,\parame,\genome_i)
    }
\end{align}
\end{subequations}
When we want constant parameters across the fitting of multiple control genomes an additional term is added to the minimization. 
\begin{equation}
    \vec{\paramm^*},\vec{\parame^*}
    \equiv 
    \arg\min_{\paramm,\parame}
    \left(
        \sum_{\genome_i}
        \gnorm(\paramm^i,\parame^i,\genome_i)
    \right)
    +
    VAR(\vec{\paramm},\vec{\parame})
\end{equation}
Where $\vec{\paramm^*},\vec{\parame^*}$ is the full set of parameters and $\paramm^i,\parame^i$ are the parameters fitted to $\genome_i$.

\subsection{Movement Model}
\begin{figure*}
    \centering
    \includegraphics[width=0.75\linewidth]{img/model_primitives}
    \caption{Planning primitives. K values indicate parameters subject to optimization.}
    \label{fig:movement:prims}
\end{figure*}
In this section we will describe how movement can be generated from Control Genomes using a Movement Model. Without loss of generality, we will examine plane locomotion and limit the output of the model to trajectories, which is time series of connected positions and facing directions. 
Conceptually there are no limitations on the complexity of the models we chose. Dense neural networks [ref] or muscle based physical simulations [ref] are capable of even generating full body locomotion. However we would like a model that exposes easily and exactly tweakable parameters to the game designers and animators. In many game scenarios micro timings and the 'feel' of the character movement are core parts of the user experience. Accordingly we need descriptive models, that can still be transparently manipulated by the artists. 

We use the trajectory as evaluation space with $\pos,\facing$ as the 2D plane position and 1D rotation around the up-axis over $t$ frames. Hence we have dimension $\dimes = 3 t$ for our evaluation space,
\begin{align}
\mathcal{R}^{3 t} 
\equiv 
\left\{
    \pos_0,\facing_0,
    \ldots,
    \pos_t,\facing_t
    \right\}
\end{align}
We construct Control Genomes as an initial state followed by a sequence of movement characteristics we would like to achieve. Player input is supplied through standard gamepad control using stick direction and button presses with $\left\{ \controlface_d, \controlmove_d\right\}$ representing the desired facing and movement directions at time $d$. Further parameters can be added to support different movement styles and are controlled with button presses. The simplest genome would contain an initial state and a single control input: $\genome^5 \equiv \left\{ \pos_0,\facing_0,\controlmove,\facing^c \right\}$. Addional data can be added to the Control Genomes either by expanding the data contained in the initial state, by adding additional control values or by providing a time series of control values which are then treated as a step curve. A non-parametric Movement Model maps from Control Genome to evaluation space
\begin{equation}
    \model(\goals^5)
    \rightarrow
    \underbrace{
    \left\{
    \pos_0^m,\facing_0^m,\ldots,\pos_t^m,\facing_t^m\right\}
    }_{\in \mathcal{R}^{3t}}
    \,,
\end{equation}
and is easily described as recursive updates to the initial genome state. If the state is expanded with additional values those are also integrated by the Model. 
We may write this in an abstract notation by using a generic update function $\mathcal{U}$ as follows,
\begin{subequations}
\begin{align}
    \left\{
    \pos_d^m,\facing_d^m
    \right\}
    &\leftarrow
    \left\{
    \pos_0,\facing_0    
    \right\},&d=0
    \\
    \left\{
    \pos_{d+1}^m,\facing_{d+1}^m
    \right\}
    &\leftarrow
    \mathcal{U}
    (
    \left\{
    \pos_d^m,\facing_d^m,
    \controlmove,
    \controlface,
    \dt
    \right\}
    )
    \label{eq:move:update},&d>0
\end{align}
\end{subequations}
The update acts as motion planning by controlling how the current state transitions towards a new state as defined by the Control Genome. We impose a restraint to model this transition process according to the characteristics of the reference animation, by parameterising $\model$ and optimizing using the objective function \eqref{eq:optim:single}. 

It is by no means trivial to construct a general $\model$ even for plane locomotion. Instead we opt for a modular approach where the planning function in \eqref{eq:move:update} is a composition of planning primitives that can be arranged for different levels of expressiveness. Each primitive exposes a set of adjustable parameters and performs input to output mapping using various interpolation methods. In the limit the compositions could approximate full neural networks, but should be kept simple enough for human manipulation of each parameter while maintaining the capability to model the movement in the reference animations.     

Fig. \ref{fig:movement:prims} illustrates the 3 planning primitives we use to model plane locomotion, where $\theta^d$ indicate $d$-dimensional values that can be optimized. Our primitives in this work are:

\begin{itemize}
\item{\bf Spring primitive:} We use a critically damped spring primitive that depends only on one parameter $\theta^1 \in \Re_+$. Given input $x \in \Re$ we can write the primitive mathematically as a mapping to the output $y \in \Re$ as $y \leftarrow \spring{( x , \dot{x}, \theta^1, x^\prime, \dt)}$. Here $\theta^1$ exposes a spring coefficient used to control the drag of the input variable towards the target $x^\prime$. It is essentially a time-integration over $\dt$ of a spring force with initial conditions $x$ and $\dot{x}$.

\item{\bf 1D interpolation:} The 1D-Map interpolation map uses $\theta^d$ with $d>$ data-values to find the interpolated value $y \in \Re$ of input $x \in \Re$. We write this as  $y \leftarrow \mapo(x,\theta^{d})$. With out loss of generality we choose to use spline interpolation over $k$ knots \kenny{what is $k$?}.

\item{\bf 2D interpolation:} The 2D-Map interpolation is similar defined. Except it maps input $x_0,x_1 \in \Re$ to output $y_0,y_1 \in \Re$ $(y_0,y_1) \leftarrow \mapt( (x_0, x_1),\theta^{d})$. In our implementation we use barycentric coordinates to interpolate polygon knots according to a 2D grid position given as input.    
\end{itemize}

As an example we propose a Movement Model for plane locomotion typical for 3rd person computer games that strikes a compromise between simplicity and expressiveness. We use the convention that an $i$ superscript refers to intermediary values used temporarily during the model update. First the position and facing information contained in the Control Genome state is augmented with derived values $\speed_0, \move_0, \angularspeed_0$ for speed, move direction and angular speed ie. rate of change of the movement direction. Compared to $\genome^{5}$ the expanded Control Genome looks like this,
\begin{subequations}
\begin{align}
    \genome^{8} \equiv \left\{ \pos_0,\facing_0,\speed_0,\move_0,\angularspeed_0,\controlmove,\facing^c \right\}\,.
\end{align}
\end{subequations}
It has 3 additional values that the Movement Model integrates. This reflects that more context than just position and facing direction is needed to model the movement correctly. For simplicity we keep the control values fixed, but usually they will change over time.

We compute $\diffmove_d=||\move_d^m-\controlmove_d||$ as the absolute error between the state and control values for movement direction. The full parameter set is $\paramm\equiv(\param_0^9,\param_1^4,\param_2^9,\param_3^9)$. A Movement model is constructed by first describing changes related to speed.
\begin{subequations}
\begin{align}
    \speed^\intermediary&\leftarrow{}\mapt(\angularspeed_d,\diffmove_d,\param_0^9)\\
    \param^\intermediary&\leftarrow{}\mapo(\speed_d-\speed_i, \param_1^4)\\
    \speed_{d+1}&\leftarrow{}\spring(\speed_d,\speed^\intermediary,\param^\intermediary,\dt)
\end{align}
\end{subequations}
Here $\param_0$ \kenny{No superscript? why?} models planning of the target speed depending on the current angular speed and the required change to movement direction. $\param_1$ models the acceleration of the model depending on the relative change in speed.

Modeling of changes to movement and facing directions are added.
\begin{subequations}
\begin{align}
    \move_{d+1}&\leftarrow\spring(\move_d,\controlmove,\mapt(\speed_{d},\diffmove, \param_2^9),\dt)\\ 
    \facing_{d+1}&\leftarrow\spring(\facing_d,\controlface,\mapt(\speed_{d},\difffacing, \param_3^9),\dt)\\
    \pos_{d+1}&\leftarrow{}\pos_d+\speed_{d}(\cos{\move_d},\sin{\move_d})
\end{align}
\end{subequations}
Here $\param_2$ and $\param_3$ model planning of movement and facing direction dependent on the current speed and facing values and the differences to the control targets.
The mapping to evaluation space $\model(\goals^{8})\rightarrow\mathcal{R}^{3t}$ is done by performing $t$ updates to the state and extracting $\pos$ and $\facing$ at each update, and is varied by changes to $\paramm$. The process is illustrated in Fig. \ref{fig:control:modelflow}. 
\begin{figure}
    \centering
    \includegraphics[width=0.5\columnwidth]{img/model_flow.png}
    \caption{Movement Model flow. White and grey background indicate values in the Control Genome and generated by the Movement Model respectively.}
    \label{fig:control:modelflow}
\end{figure}

An analytical model like this can be of great practical use to the game developers. The parameters have direct links to relatable properties in the animation which makes it easier to tweak the behavior of the model. On the other hand, the analytical approach requires that the designers are able to adequately model the complexity of the movement. When this is not the case, we can apply machine learning techniques to discover latent properties of our animation set. 

One approach is to extract a limited latent variable set using a dense neural network and then feed the latent variables to a set a primitives which are weighted to give the final output. 
Alternatively, we could apply a hybrid approach by discovering correlations in the data by automated statistical analysis, but still allow the designer to setup the primitives and chose which parameters should be used. 

Finally we note that multiple local Movement Models can be easily combined. Transitions between models can be established using the same primitives. In practice it is often sufficient to use a single 1D blend. For instance the shift between 'run' and 'walk', which could require individual models, is usually distinct and of limited complexity and duration. 

\subsection{Motion Alignment and Filtering}
It is difficult to capture consistent animation data. Treadmills can be used to maintain constant speeds, but this poses a limitation in maneuverability. It requires accurate high frequency control to reproduce the subtle changes in speed and direction that are present even if the subjects are directed to move in a consistent manner. It is often possible to apply minor changes to the speed and direction of the recorded animation, without loss of visual quality. By allowing this, we can align an animation set with respect to speed and movement patterns. 

An adjustment curve contains pr. frame changes to rotation and speed of an animation, and is applied as modifications to the root joint of the animation. An adjustment curve containing all zeros and a single $\frac{\pi}{2}$ would for instance modify a straight walk to have a 90 degree turn over a single frame. To maintain a smooth and subtle modification we can interpolate sparsely distributed key frames and impose limitation on the amount of adjustment allowed pr. frame. We found that various heuristics such as scaling the adjustment depending the relative amount of movement at a given frame, or by the distance from a frame the to nearest control genome, helps maintain the quality of the animation (se fig for reference). Post processing techniques such as foot-IK, Paragon step length increase, or pivoting rotation around foot contacts can also be applied.

Even perfectly consistent animation data contains hard to model fluctuations due to the way humans move. [ref] illustrates the difficulty of deriving a stable position from animation data. We apply a modulating kernel based filter similar to [ref] before the application of the adjustment curve. We use $Adjust(\theta_a,Filter(\theta_f\anim^{\dimas,0}_\dimat))\rightarrow{}[\pos_0,\facing_0, \ldots \pos_t\facing_t]$ as $\edit$ in the objective function (\ref{eq:optim:single}) where $\theta_a, \theta_f$ represent the adjustment curve values and filter kernel weights respectively.

\subsection{Optimization}
When the Movement Genome, Movement Model and the alignment and filtering procedures have been defined, we face an optimization problem consisting of 100s of parameters and many local minima. Fig ?? illustrates how the parameters are distributed across the full system. \magnus{Missing: Genomes can be slightly modified. Changes are in $\theta_G$ which needs to be defined}.

The dimensionality of the parameter space makes it infeasible to use straight forward techniques such as finite differences. We attack the problem by using auto differentiation with PyTorch for the optimization procedure. Having the gradients available we can use line search techniques such as Gradient Descent or Stochastic Gradient Descent which is available in PyTorch.

To use a global learning rate, we normalize the range of all parameters and apply fixed constant scaling. This is easily doable since most parameters have direct physical counter parts such as velocity, turn angles etc. Scale values can then be identified by doing a pre-pass over the animation. This in turn limits the impact of local minima, as the optimization can be given plausible initialization values.

L1 and L2 regularization is applied to most parameters to avoid 'oscillating' interpolations between extreme knot values which can be difficult to interpret and modify for a human. Similar regularization is applied to $\theta_a$ and $\theta_G$ where large values implies divergence from the ground truth present in the Genomes and reference animation.

Auto differentiation requires continuity across the parameters space, and some care must be taken to achieve this. Specifically we ensure that our primitive function are continuous and use leaky relu outside valid ranges to support correct derivates here. 
