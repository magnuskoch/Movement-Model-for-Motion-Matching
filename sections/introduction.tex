\section{Introduction}
Realistic and responsive character animation is important for establishing immersion in computer games. The complexity of human movement makes it difficult to construct such an experience by hand. Instead we record reference data using motion capture and construct animation synthesizers that either stitches clips together, samples inferred statistical models, or learns control policies for a physical model to mimic the animations. Recent advances in both industry and research communities show a strong tendency towards complex closed systems guided by weak control signals. (use a spring to), (use a to).
To our best knowledge it has not been investigated how to optimally construct and synchronize weak control signals to full body animation synthesizers. This is important for computer games. It is an industry standard to have a Gameplay Layer that controls changes to character position and a separate Animation Layer that tries to generate animations that matches the changes in positions. We suggest the term \textit{Movement Model} to denote the Gameplay Layer logic that controls character position in response to player input. This model is carefully tuned by designers to give movement response that \textit{feels appealing}, it is use to predict future character movement and even for analyzing the validity of game levels. As such it seems an ideal control signal for animation synthesizers. We have a history of movement available and by design the models computationally cheap to integrate for predictions. But a challenge is introduced. The Movement Model is not only weak signal since but rather the exact position changes of game characters. If there is a disconnect between the Movement Model and the animation synthesizer output we might allow the animations to diverge from the Movement Model and later catch up, but this severely impacts the types of game experiences that can be created and changes the feeling of movement in response to player input. If we enforce synchronization between the two layers artifacts such as foot sliding and a general degradation of realism and quality kick in. It is an industry standard to carefully synchronize animations and movement models by hand at great economical cost.

it is a far less ambitious task to generate Movement models compared to full body animations. But we also have requiremetns to these models. Fast to evaluate (used for prediction etc. 0.0001 ms) . The define the feeling of the game. Should be possible to tweak. 

We introduce 'control genomes' as a wau to regularize the task at hand. Then we suggest some primitives that can be used to build models. And finally we use aniamtion editing techniques in the optimizaation to align the data set . We also discovered a novel way to determine movement in animations based of contact detection that can be of use in its own right.



and   the animation and gameplay code.

Most games calculate changes to character position 

in a gameplay layer which is kept completely isolated from the animation system. 


and the animation layer which produces full body animation to match the movement  





Most game implementations keep strict division between animation and gameplay systems, where the latter infers changes to character position from user input using what we will refer to as the \textit{Movement Model}. By concateniting a history and future predection , we generate trajectoris that can be used as weak control signal for the black box animation synthesizers. But we know have a problem. Signal is no longer weak. It is the exact movement of the charcter. Discrepancy will decrease quality. 

Most games implement a self contained system to control character position independently 
w a system to change character position based on user input. 

In many game productions we have a logical to control the character position from user input. This signal is used as weak control for animation synthesizer. This is ok if we allow the system to drift away from our weak signal. But it becomes a problem if we want the system to precisly follow our guide. CAn be seen in many games. If responsiveness is important, we see lots of sliding (disconnect between movement model and animation synthsizer.)


Disconnect (animatjon and our knowledge of how the system will move)






Both research and industry communities p
Character animation for computer games has embraced a data driven approach in both industry [ref] and research communities [ref]. Animation synthesizers or control policies  



Computer games are growing. Both in scope and ambition. Virtual worlds are becoming larger and contain increasingly complex and dynamic interactions between the characters and the environment. Multiplayer game play is most commonly the norm, and requires that the internal state of the rapidly changing game world can be reliable passed around and kept synchronized between players around the world. To manage the complexity of interaction of the full game world, individual game components are usually treated as abstract models, that should be easy to reason about, to update and keep synchronised. Then as a next step more fine grained models or simply visual fidelity is added on top. 

While it has proven extremely difficult to synthesize dynamic animations of human characters, the challenge is only greater in the context of modern game production. A common approach is to split the animation system into a abstract movement model, which produces the overall state of the game characters, and an animation system which generates animations that realistically and closely follow the path of the movement model. Introduced in 2015 Motion Matching has been adopted by many studios \kenny{as evidenced by games such ass XX, YY, ZZ}. At its core, the system stitches animations streams together by continually transitioning between frames in a large database of animation clips. The transitions are determined by finding the best matches to an external requirement which is naturally provided by the movement model. Motion matching excels when the animation database has a substantial coverage allowing the system to generate a full range of animations that are still close to the ground truth \kenny{Can we add some cites for this claim? Or we can rephrase it as: "In our experience Motion Matching excels...."}. 

Conceptually the movement model provides a control signal into the motion database. To keep synthesized animation realistic and fluent, it is critical that this control signal is reproducible in the database. In practice this is often not the case, and the issue is addressed by manually tweaking all exposed parameters in the pipeline. Most importantly weighted heuristics are added to the motion matching system, animation trajectories are manually edited and the movement model is set to reproduce the main movement modes present in the animations. To our best knowledge the process of tweaking poses is the biggest challenge to the practical use of motion matching.

In this paper we formulate the synchronization of the motion matching system, an animation database and movement model as an ill posed optimization procedure. We suggest a number of regularization steps, arriving at a solution that enables us to use unstructured animations for motion matching under movement model control while preserving fidelity in the database. The regularization procedure has two steps, where we first identify main modes in the animation database, and then distribute a sparse user control along all animation in the database. Manual edits to the animations is replaced by a warping system, allowing us to optimize against a wider range of valid input animations. Finally an optimization procedure is applied to the partially constrained movement model and animations to arrive at a configuration of the entire system.

Our contribution is a conceptual formulation of the movement model, the regularization steps required to run the optimization procedure and the [something] required to make optimization work nicely.
