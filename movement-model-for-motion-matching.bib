@article{Bergamin19,
author = {Bergamin, Kevin and Clavet, Simon and Holden, Daniel and Forbes, James Richard},
title = {DReCon: Data-Driven Responsive Control of Physics-Based Characters},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3355089.3356536},
doi = {10.1145/3355089.3356536},
abstract = {Interactive control of self-balancing, physically simulated humanoids is a long standing problem in the field of real-time character animation. While physical simulation guarantees realistic interactions in the virtual world, simulated characters can appear unnatural if they perform unusual movements in order to maintain balance. Therefore, obtaining a high level of responsiveness to user control, runtime performance, and diversity has often been overlooked in exchange for motion quality. Recent work in the field of deep reinforcement learning has shown that training physically simulated characters to follow motion capture clips can yield high quality tracking results. We propose a two-step approach for building responsive simulated character controllers from unstructured motion capture data. First, meaningful features from the data such as movement direction, heading direction, speed, and locomotion style, are interactively specified and drive a kinematic character controller implemented using motion matching. Second, reinforcement learning is used to train a simulated character controller that is general enough to track the entire distribution of motion that can be generated by the kinematic controller. Our design emphasizes responsiveness to user input, visual quality, and low runtime cost for application in video-games.},
journal = {ACM Trans. Graph.},
month = nov,
articleno = {206},
numpages = {11},
keywords = {physically based animation, real-time graphics, reinforcement learning, motion capture}
}
@article{buttner20,
  author          = {Michael Buttner},
  title           = {Kinematica: democratizing Motion Matching for character animation},
  year            = {2020},
  url             = {https://www.youtube.com/watch?v=TlIWPS2Wzac}
}

@article{kermse.04,
author = {Andrew Kermse},
title = {Game Programming Gems 4},
year = {2004},
numpages = {95-101}
}

@article{holden.16,
author = {Holden, Daniel and Saito, Jun and Komura, Taku},
title = {A Deep Learning Framework for Character Motion Synthesis and Editing},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/2897824.2925975},
doi = {10.1145/2897824.2925975},
abstract = {We present a framework to synthesize character movements based on high level parameters, such that the produced movements respect the manifold of human motion, trained on a large motion capture dataset. The learned motion manifold, which is represented by the hidden units of a convolutional autoencoder, represents motion data in sparse components which can be combined to produce a wide range of complex movements. To map from high level parameters to the motion manifold, we stack a deep feedforward neural network on top of the trained autoencoder. This network is trained to produce realistic motion sequences from parameters such as a curve over the terrain that the character should follow, or a target location for punching and kicking. The feedforward control network and the motion manifold are trained independently, allowing the user to easily switch between feedforward networks according to the desired interface, without re-training the motion manifold. Once motion is generated it can be edited by performing optimization in the space of the motion manifold. This allows for imposing kinematic constraints, or transforming the style of the motion, while ensuring the edited motion remains natural. As a result, the system can produce smooth, high quality motion sequences without any manual pre-processing of the training data.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {138},
numpages = {11},
keywords = {convolutional neural networks, deep learning, character animation, manifold learning, autoencoder, human motion}
}

@article{holden.ea17,
author = {Holden, Daniel and Komura, Taku and Saito, Jun},
title = {Phase-Functioned Neural Networks for Character Control},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3072959.3073663},
doi = {10.1145/3072959.3073663},
abstract = {We present a real-time character control mechanism using a novel neural network architecture called a Phase-Functioned Neural Network. In this network structure, the weights are computed via a cyclic function which uses the phase as an input. Along with the phase, our system takes as input user controls, the previous state of the character, the geometry of the scene, and automatically produces high quality motions that achieve the desired user control. The entire network is trained in an end-to-end fashion on a large dataset composed of locomotion such as walking, running, jumping, and climbing movements fitted into virtual environments. Our system can therefore automatically produce motions where the character adapts to different geometric environments such as walking and running over rough terrain, climbing over large rocks, jumping over obstacles, and crouching under low ceilings. Our network architecture produces higher quality results than time-series autoregressive models such as LSTMs as it deals explicitly with the latent variable of motion relating to the phase. Once trained, our system is also extremely fast and compact, requiring only milliseconds of execution time and a few megabytes of memory, even when trained on gigabytes of motion data. Our work is most appropriate for controlling characters in interactive scenes such as computer games and virtual reality systems.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {42},
numpages = {13},
keywords = {human motion, neural networks, character animation, character control, deep learning, locomotion}
}

@article{holden18,
  title={Character Control with Neural Networks and Machine Learning.},
  author={Daniel Holden},
  journal = {Proc. of GDC 2018},
  year={2018}
}

@article{holden.ea20,
author = {Holden, Daniel and Kanoun, Oussama and Perepichka, Maksym and Popa, Tiberiu},
title = {Learned Motion Matching},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3386569.3392440},
doi = {10.1145/3386569.3392440},
abstract = {In this paper we present a learned alternative to the Motion Matching algorithm which retains the positive properties of Motion Matching but additionally achieves the scalability of neural-network-based generative models. Although neural-network-based generative models for character animation are capable of learning expressive, compact controllers from vast amounts of animation data, methods such as Motion Matching still remain a popular choice in the games industry due to their flexibility, predictability, low preprocessing time, and visual quality - all properties which can sometimes be difficult to achieve with neural-network-based methods. Yet, unlike neural networks, the memory usage of such methods generally scales linearly with the amount of data used, resulting in a constant trade-off between the diversity of animation which can be produced and real world production budgets. In this work we combine the benefits of both approaches and, by breaking down the Motion Matching algorithm into its individual steps, show how learned, scalable alternatives can be used to replace each operation in turn. Our final model has no need to store animation data or additional matching meta-data in memory, meaning it scales as well as existing generative models. At the same time, we preserve the behavior of Motion Matching, retaining the quality, control, and quick iteration time which are so important in the industry.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {53},
numpages = {13},
keywords = {motion matching, neural networks, animation, generative models, character animation}
}

@article{holden21,
  author          = {Daniel Holden},
  title           = {http://theorangeduck.com/page/spring-roll-call},
  url             = {http://theorangeduck.com/page/spring-roll-call},
}

@article{kovar02,
author = {Kovar, Lucas and Gleicher, Michael and Pighin, Fr\'{e}d\'{e}ric},
title = {Motion Graphs},
year = {2002},
issue_date = {July 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {0730-0301},
url = {https://doi.org/10.1145/566654.566605},
doi = {10.1145/566654.566605},
abstract = {In this paper we present a novel method for creating realistic, controllable motion. Given a corpus of motion capture data, we automatically construct a directed graph called a motion graph that encapsulates connections among the database. The motion graph consists both of pieces of original motion and automatically generated transitions. Motion can be generated simply by building walks on the graph. We present a general framework for extracting particular graph walks that meet a user's specifications. We then show how this framework can be applied to the specific problem of generating different styles of locomotion along arbitrary paths.},
journal = {ACM Trans. Graph.},
month = jul,
pages = {473â€“482},
numpages = {10},
keywords = {motion capture, motion synthesis, animation with constraints}
}

@article{lee03,
author = {Lee, J. and Chai, Jinxiang and Reitsma, Paul and Hodgins, Jessica and Pollard, Nancy},
year = {2003},
month = {07},
pages = {},
title = {Interactive Control of Avatars Animated with Human Motion Data},
volume = {21},
journal = {ACM Transactions on Graphics (SIGGRAPH 2002)},
doi = {10.1145/566570.566607}
}

@article{lee10,
author = {Lee, Yongjoon and Wampler, Kevin and Bernstein, Gilbert and Popovi\'{c}, Jovan and Popovi\'{c}, Zoran},
title = {Motion Fields for Interactive Character Locomotion},
year = {2010},
issue_date = {December 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/1882261.1866160},
doi = {10.1145/1882261.1866160},
abstract = {We propose a novel representation of motion data and control that enables characters with both highly agile responses to user input and natural handling of arbitrary external disturbances. The representation organizes motion data as samples in a high dimensional generalization of a vector field we call a 'motion field'. Our runtime motion synthesis mechanism freely 'flows' in the motion field and is capable of creating novel and natural motions that are highly-responsive to the real time user input, and generally not explicitly specified in the data.},
journal = {ACM Trans. Graph.},
month = dec,
articleno = {138},
numpages = {8},
keywords = {animation, data-driven animation, motion representation}
}


@article{lee18,
author = {Lee, Kyungho and Lee, Seyoung and Lee, Jehee},
title = {Interactive Character Animation by Learning Multi-Objective Control},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3272127.3275071},
doi = {10.1145/3272127.3275071},
abstract = {We present an approach that learns to act from raw motion data for interactive character animation. Our motion generator takes a continuous stream of control inputs and generates the character's motion in an online manner. The key insight is modeling rich connections between a multitude of control objectives and a large repertoire of actions. The model is trained using Recurrent Neural Network conditioned to deal with spatiotemporal constraints and structural variabilities in human motion. We also present a new data augmentation method that allows the model to be learned even from a small to moderate amount of training data. The learning process is fully automatic if it learns the motion of a single character, and requires minimal user intervention if it deals with props and interaction between multiple characters.},
journal = {ACM Trans. Graph.},
month = dec,
articleno = {180},
numpages = {10},
keywords = {deep learning, multi-objective control, interactive motion control, recurrent neural network, character animation, motion grammar}
}

@inproceedings{mccann07,
author = {McCann, James and Pollard, Nancy},
title = {Responsive Characters from Motion Fragments},
year = {2007},
isbn = {9781450378369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1275808.1276385},
doi = {10.1145/1275808.1276385},
abstract = {In game environments, animated character motion must rapidly adapt to changes in player input - for example, if a directional signal from the player's gamepad is not incorporated into the character's trajectory immediately, the character may blithely run off a ledge. Traditional schemes for data-driven character animation lack the split-second reactivity required for this direct control; while they can be made to work, motion artifacts will result. We describe an on-line character animation controller that assembles a motion stream from short motion fragments, choosing each fragment based on current player input and the previous fragment. By adding a simple model of player behavior we are able to improve an existing reinforcement learning method for precalculating good fragment choices. We demonstrate the efficacy of our model by comparing the animation selected by our new controller to that selected by existing methods and to the optimal selection, given knowledge of the entire path. This comparison is performed over real-world data collected from a game prototype. Finally, we provide results indicating that occasional low-quality transitions between motion segments are crucial to high-quality on-line motion generation; this is an important result for others crafting animation systems for directly-controlled characters, as it argues against the common practice of transition thresholding.},
booktitle = {ACM SIGGRAPH 2007 Papers},
pages = {6â€“es},
keywords = {character control, motion generation, motion graphs},
location = {San Diego, California},
series = {SIGGRAPH '07}
}

@article{peng17,
  title={DeepLoco: Dynamic Locomotion Skills Using Hierarchical Deep Reinforcement Learning},
  author={Xue Bin Peng and Glen Berseth and KangKang Yin and Michiel van de Panne},
  journal = {ACM Transactions on Graphics (Proc. SIGGRAPH 2017)},
  volume = 36,
  number = 4,
  article = 41,
  year={2017}
}

@article{safanona07,
author = {Safonova, Alla and Hodgins, Jessica K.},
title = {Construction and Optimal Search of Interpolated Motion Graphs},
year = {2007},
issue_date = {July 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {3},
issn = {0730-0301},
url = {https://doi.org/10.1145/1276377.1276510},
doi = {10.1145/1276377.1276510},
abstract = {Many compelling applications would become feasible if novice users had the ability to synthesize high quality human motion based only on a simple sketch and a few easily specified constraints. We approach this problem by representing the desired motion as an interpolation of two time-scaled paths through a motion graph. The graph is constructed to support interpolation and pruned for efficient search. We use an anytime version of A* search to find a globally optimal solution in this graph that satisfies the user's specification. Our approach retains the natural transitions of motion graphs and the ability to synthesize physically realistic variations provided by interpolation. We demonstrate the power of this approach by synthesizing optimal or near optimal motions that include a variety of behaviors in a single motion.},
journal = {ACM Trans. Graph.},
month = jul,
pages = {106â€“es},
numpages = {12},
keywords = {motion graph, human animation, motion interpolation, motion capture, motion planning}
}

  



@article{startke20,
author = {Starke, Sebastian and Zhao, Yiwei and Komura, Taku and Zaman, Kazi},
title = {Local Motion Phases for Learning Multi-Contact Character Movements},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3386569.3392450},
doi = {10.1145/3386569.3392450},
abstract = {Training a bipedal character to play basketball and interact with objects, or a quadruped character to move in various locomotion modes, are difficult tasks due to the fast and complex contacts happening during the motion. In this paper, we propose a novel framework to learn fast and dynamic character interactions that involve multiple contacts between the body and an object, another character and the environment, from a rich, unstructured motion capture database. We use one-on-one basketball play and character interactions with the environment as examples. To achieve this task, we propose a novel feature called local motion phase, that can help neural networks to learn asynchronous movements of each bone and its interaction with external objects such as a ball or an environment. We also propose a novel generative scheme to reproduce a wide variation of movements from abstract control signals given by a gamepad, which can be useful for changing the style of the motion under the same context. Our scheme is useful for animating contact-rich, complex interactions for real-time applications such as computer games.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {54},
numpages = {14},
keywords = {character animation, human motion, deep learning, character control, character interactions, neural networks}
}

@inproceedings{treuille07,
author = {Treuille, Adrien and Lee, Yongjoon and Popovi\'{c}, Zoran},
title = {Near-Optimal Character Animation with Continuous Control},
year = {2007},
isbn = {9781450378369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1275808.1276386},
doi = {10.1145/1275808.1276386},
abstract = {We present a new approach to realtime character animation with interactive control. Given a corpus of motion capture data and a desired task, we automatically compute near-optimal controllers using a low-dimensional basis representation. We show that these controllers produce motion that fluidly responds to several dimensions of user control and environmental constraints in realtime. Our results indicate that very few basis functions are required to create high-fidelity character controllers which permit complex user navigation and obstacle-avoidance tasks.},
booktitle = {ACM SIGGRAPH 2007 Papers},
pages = {7â€“es},
keywords = {optimal control, human animation, motion with constraints},
location = {San Diego, California},
series = {SIGGRAPH '07}
}

@article{zhang18,
author = {zhang, He and Starke, Sebastian and Komura, Taku and Saito, Jun},
title = {Mode-Adaptive Neural Networks for Quadruped Motion Control},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3197517.3201366},
doi = {10.1145/3197517.3201366},
abstract = {Quadruped motion includes a wide variation of gaits such as walk, pace, trot and canter, and actions such as jumping, sitting, turning and idling. Applying existing data-driven character control frameworks to such data requires a significant amount of data preprocessing such as motion labeling and alignment. In this paper, we propose a novel neural network architecture called Mode-Adaptive Neural Networks for controlling quadruped characters. The system is composed of the motion prediction network and the gating network. At each frame, the motion prediction network computes the character state in the current frame given the state in the previous frame and the user-provided control signals. The gating network dynamically updates the weights of the motion prediction network by selecting and blending what we call the expert weights, each of which specializes in a particular movement. Due to the increased flexibility, the system can learn consistent expert weights across a wide range of non-periodic/periodic actions, from unstructured motion capture data, in an end-to-end fashion. In addition, the users are released from performing complex labeling of phases in different gaits. We show that this architecture is suitable for encoding the multi-modality of quadruped locomotion and synthesizing responsive motion in real-time.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {145},
numpages = {11},
keywords = {locomotion, neural networks, deep learning, character control, human motion, character animation}
}

  


  

